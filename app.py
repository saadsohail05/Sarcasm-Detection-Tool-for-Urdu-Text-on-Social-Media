import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import urduhack
from urduhack.preprocessing import normalize_whitespace, remove_punctuation, remove_accents
import demoji
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
from sklearn.model_selection import train_test_split
from nltk import ngrams
from collections import Counter 
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import streamlit as st
import joblib

# 1. Set page configuration as the first Streamlit command
st.set_page_config(
    page_title="Urdu Sarcasm Detection",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# 2. Custom CSS for centering content and styling
st.markdown("""
    <style>
    /* Center the title, subheader, and input box */
    .centered-content {
        display: flex;
        justify-content: center;
        align-items: center;
        flex-direction: column;
    }

    /* Style the buttons */
    .stButton > button {
        background-color: #00aaff;
        color: white;
        border-radius: 10px;
        width: 200px;
        margin: 20px auto;
        display: block;
    }

    /* Style for title and headers */
    h1, h2, h3 {
        text-align: center;
        color: #00aaff;
    }

    /* Style for body background */
    body {
        background-color: #f0f2f6;
    }

    /* Progress bar style */
    .stProgress > div > div > div {
        background-color: #00aaff;
    }

    </style>
""", unsafe_allow_html=True)

# 3. Title and description with centered layout
st.markdown('<div class="centered-content">', unsafe_allow_html=True)
st.title("ğŸ§  Urdu Sarcasm Detection")
st.subheader("Analyze Urdu text for sarcasm with AI-powered detection")
st.markdown('</div>', unsafe_allow_html=True)

# 4. Loading Stopwords
with open("stopwords-ur.txt", 'r', encoding='utf-8') as file:
    stopwords_from_file = file.read().splitlines()

stopdf = pd.read_csv("urdu_stopwords.csv", encoding='utf-8')
stopwords_from_csv = stopdf["stopword"].tolist()

# Additional stopwords
stop_words = set("""
Ø¢ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒÚº Ø¢Ø¦Û’ Ø¢ØªØ§ Ø¢ØªÛŒ Ø¢ØªÛ’ Ø¢Ø¯Ø§Ø¨ Ø¢Ø¯Ú¾ Ø¢Ø¯Ú¾Ø§ Ø¢Ø¯Ú¾ÛŒ Ø¢Ø¯Ú¾Û’ Ø¢Ø³ Ø¢Ù…Ø¯ÛŒØ¯ Ø¢Ù†Ø§ Ø¢Ù†Ø³Û Ø¢Ù†ÛŒ Ø¢Ù†Û’
Ø¢Ù¾ Ø¢Ú¯Û’ Ø¢Û Ø¢ÛØ§ Ø¢ÛŒØ§ Ø§Ø¨ Ø§Ø¨Ú¾ÛŒ Ø§Ø¨Û’ Ø§ØªÙˆØ§Ø± Ø§Ø±Ø¨ Ø§Ø±Ø¨ÙˆÛŒÚº Ø§Ø±Û’ Ø§Ø³ Ø§Ø³Ú©Ø§ Ø§Ø³Ú©ÛŒ Ø§Ø³Ú©Û’ Ø§Ø³ÛŒ Ø§Ø³Û’ Ø§Ù
Ø§ÙÙˆÛ Ø§Ù„Ø§ÙˆÙ„ Ø§Ù„Ø¨ØªÛ Ø§Ù„Ø«Ø§Ù†ÛŒ Ø§Ù„Ø­Ø±Ø§Ù… Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù Ø§Ù„Ù…Ú©Ø±Ù… Ø§Ù† Ø§Ù†Ø¯Ø± Ø§Ù†Ú©Ø§ Ø§Ù†Ú©ÛŒ Ø§Ù†Ú©Û’ Ø§Ù†ÛÙˆÚº Ø§Ù†ÛÛŒ Ø§Ù†ÛÛŒÚº Ø§ÙˆØ¦Û’ Ø§ÙˆØ±
Ø§ÙˆÙ¾Ø± Ø§ÙˆÛÙˆ Ø§Ù¾ Ø§Ù¾Ù†Ø§ Ø§Ù¾Ù†ÙˆÚº Ø§Ù¾Ù†ÛŒ Ø§Ù¾Ù†Û’ Ø§Ù¾Ù†Û’Ø¢Ù¾ Ø§Ú©Ø¨Ø± Ø§Ú©Ø«Ø± Ø§Ú¯Ø± Ø§Ú¯Ø±Ú†Û Ø§Ú¯Ø³Øª Ø§ÛØ§ÛØ§ Ø§ÛŒØ³Ø§ Ø§ÛŒØ³ÛŒ Ø§ÛŒØ³Û’ Ø§ÛŒÚ© Ø¨Ø§Ø¦ÛŒÚº
Ø¨Ø§Ø± Ø¨Ø§Ø±Û’ Ø¨Ø§Ù„Ú©Ù„ Ø¨Ø§ÙˆØ¬ÙˆØ¯ Ø¨Ø§ÛØ± Ø¨Ø¬ Ø¨Ø¬Û’ Ø¨Ø®ÛŒØ± Ø¨Ø±Ø³Ø§Øª Ø¨Ø´Ø±Ø·ÛŒÚ©Û Ø¨Ø¹Ø¯ Ø¨Ø¹Ø¶ Ø¨ØºÛŒØ± Ø¨Ù„Ú©Û Ø¨Ù† Ø¨Ù†Ø§ Ø¨Ù†Ø§Ø¤ Ø¨Ù†Ø¯
Ø¨Ú‘ÛŒ Ø¨Ú¾Ø± Ø¨Ú¾Ø±ÛŒÚº Ø¨Ú¾ÛŒ Ø¨ÛØ§Ø± Ø¨ÛØª Ø¨ÛØªØ± Ø¨ÛŒÚ¯Ù… ØªØ§Ú©Û ØªØ§ÛÙ… ØªØ¨ ØªØ¬Ú¾ ØªØ¬Ú¾ÛŒ ØªØ¬Ú¾Û’ ØªØ±Ø§ ØªØ±ÛŒ ØªÙ„Ú© ØªÙ… ØªÙ…Ø§Ù…
ØªÙ…ÛØ§Ø±Ø§ ØªÙ…ÛØ§Ø±ÙˆÚº ØªÙ…ÛØ§Ø±ÛŒ ØªÙ…ÛØ§Ø±Û’ ØªÙ…ÛÛŒÚº ØªÙˆ ØªÚ© ØªÚ¾Ø§ ØªÚ¾ÛŒ ØªÚ¾ÛŒÚº ØªÚ¾Û’ ØªÛØ§Ø¦ÛŒ ØªÛŒØ±Ø§ ØªÛŒØ±ÛŒ ØªÛŒØ±Û’ ØªÛŒÙ† Ø¬Ø§ Ø¬Ø§Ø¤
Ø¬Ø§Ø¦ÛŒÚº Ø¬Ø§Ø¦Û’ Ø¬Ø§ØªØ§ Ø¬Ø§ØªÛŒ Ø¬Ø§ØªÛ’ Ø¬Ø§Ù†ÛŒ Ø¬Ø§Ù†Û’ Ø¬Ø¨ Ø¬Ø¨Ú©Û Ø¬Ø¯Ú¾Ø± Ø¬Ø³ Ø¬Ø³Û’ Ø¬Ù† Ø¬Ù†Ø§Ø¨ Ø¬Ù†ÛÙˆÚº Ø¬Ù†ÛÛŒÚº Ø¬Ùˆ Ø¬ÛØ§Úº Ø¬ÛŒ
Ø¬ÛŒØ³Ø§ Ø¬ÛŒØ³ÙˆÚº Ø¬ÛŒØ³ÛŒ Ø¬ÛŒØ³Û’ Ø¬ÛŒÙ¹Ú¾ Ø­Ø§Ù„Ø§Ù†Ú©Û Ø­Ø§Ù„Ø§Úº Ø­ØµÛ Ø­Ø¶Ø±Øª Ø®Ø§Ø·Ø± Ø®Ø§Ù„ÛŒ Ø®Ø¯Ø§ Ø®Ø²Ø§Úº Ø®ÙˆØ§Û Ø®ÙˆØ¨ Ø®ÙˆØ¯ Ø¯Ø§Ø¦ÛŒÚº Ø¯Ø±Ù…ÛŒØ§Ù†
Ø¯Ø±ÛŒÚº Ø¯Ùˆ Ø¯ÙˆØ±Ø§Ù† Ø¯ÙˆØ³Ø±Ø§ Ø¯ÙˆØ³Ø±ÙˆÚº Ø¯ÙˆØ³Ø±ÛŒ Ø¯ÙˆØ´Ù†Ø¨Û Ø¯ÙˆÚº Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ø¯Ú¯Ù†Ø§ Ø¯ÛŒ Ø¯ÛŒØ¦Û’ Ø¯ÛŒØ§ Ø¯ÛŒØªØ§ Ø¯ÛŒØªÛŒ Ø¯ÛŒØªÛ’ Ø¯ÛŒØ± Ø¯ÛŒÙ†Ø§ Ø¯ÛŒÙ†ÛŒ
Ø¯ÛŒÙ†Û’ Ø¯ÛŒÚ©Ú¾Ùˆ Ø¯ÛŒÚº Ø¯ÛŒÛ’ Ø¯Û’ Ø°Ø±ÛŒØ¹Û’ Ø±Ú©Ú¾Ø§ Ø±Ú©Ú¾ØªØ§ Ø±Ú©Ú¾ØªÛŒ Ø±Ú©Ú¾ØªÛ’ Ø±Ú©Ú¾Ù†Ø§ Ø±Ú©Ú¾Ù†ÛŒ Ø±Ú©Ú¾Ù†Û’ Ø±Ú©Ú¾Ùˆ Ø±Ú©Ú¾ÛŒ Ø±Ú©Ú¾Û’ Ø±Û Ø±ÛØ§
Ø±ÛØªØ§ Ø±ÛØªÛŒ Ø±ÛØªÛ’ Ø±ÛÙ†Ø§ Ø±ÛÙ†ÛŒ Ø±ÛÙ†Û’ Ø±ÛÙˆ Ø±ÛÛŒ Ø±ÛÛŒÚº Ø±ÛÛ’ Ø³Ø§ØªÚ¾ Ø³Ø§Ù…Ù†Û’ Ø³Ø§Ú‘Ú¾Û’ Ø³Ø¨ Ø³Ø¨Ú¾ÛŒ Ø³Ø±Ø§Ø³Ø± Ø³Ù„Ø§Ù… Ø³Ù…ÛŒØª Ø³ÙˆØ§
Ø³ÙˆØ§Ø¦Û’ Ø³Ú©Ø§ Ø³Ú©ØªØ§ Ø³Ú©ØªÛ’ Ø³Û Ø³ÛÛŒ Ø³ÛŒ Ø³Û’ Ø´Ø§Ù… Ø´Ø§ÛŒØ¯ Ø´Ú©Ø±ÛŒÛ ØµØ§Ø­Ø¨ ØµØ§Ø­Ø¨Û ØµØ±Ù Ø¶Ø±ÙˆØ± Ø·Ø±Ø­ Ø·Ø±Ù Ø·ÙˆØ±
Ø¹Ù„Ø§ÙˆÛ Ø¹ÛŒÙ† ÙØ±ÙˆØ±ÛŒ ÙÙ‚Ø· ÙÙ„Ø§Úº ÙÛŒ Ù‚Ø¨Ù„ Ù‚Ø·Ø§ Ù„Ø¦Û’ Ù„Ø§Ø¦ÛŒ Ù„Ø§Ø¦Û’ Ù„Ø§ØªØ§ Ù„Ø§ØªÛŒ Ù„Ø§ØªÛ’ Ù„Ø§Ù†Ø§ Ù„Ø§Ù†ÛŒ Ù„Ø§Ù†Û’ Ù„Ø§ÛŒØ§ Ù„Ùˆ
Ù„ÙˆØ¬ÛŒ Ù„ÙˆÚ¯ÙˆÚº Ù„Ú¯ Ù„Ú¯Ø§ Ù„Ú¯ØªØ§ Ù„Ú¯ØªÛŒ Ù„Ú¯ÛŒ Ù„Ú¯ÛŒÚº Ù„Ú¯Û’ Ù„ÛØ°Ø§ Ù„ÛŒ Ù„ÛŒØ§ Ù„ÛŒØªØ§ Ù„ÛŒØªÛŒ Ù„ÛŒØªÛ’ Ù„ÛŒÚ©Ù† Ù„ÛŒÚº Ù„ÛŒÛ’
Ù„Û’ Ù…Ø§Ø³ÙˆØ§ Ù…Øª Ù…Ø¬Ú¾ Ù…Ø¬Ú¾ÛŒ Ù…Ø¬Ú¾Û’ Ù…Ø­ØªØ±Ù… Ù…Ø­ØªØ±Ù…Û Ù…Ø­ØªØ±Ù…ÛŒ Ù…Ø­Ø¶ Ù…Ø±Ø§ Ù…Ø±Ø­Ø¨Ø§ Ù…Ø±ÛŒ Ù…Ø±Û’ Ù…Ø²ÛŒØ¯ Ù…Ø³ Ù…Ø³Ø² Ù…Ø³Ù¹Ø± Ù…Ø·Ø§Ø¨Ù‚
Ù…Ø·Ù„Ù‚ Ù…Ù„ Ù…Ù†Ù¹ Ù…Ù†Ù¹ÙˆÚº Ù…Ú©Ø±Ù…ÛŒ Ù…Ú¯Ø± Ù…Ú¯Ú¾Ø± Ù…ÛØ±Ø¨Ø§Ù†ÛŒ Ù…ÛŒØ±Ø§ Ù…ÛŒØ±ÙˆÚº Ù…ÛŒØ±ÛŒ Ù…ÛŒØ±Û’ Ù…ÛŒÚº Ù†Ø§ Ù†Ø²Ø¯ÛŒÚ© Ù†Ù…Ø§ Ù†Ùˆ Ù†ÙˆÙ…Ø¨Ø±
Ù†Û Ù†ÛÛŒÚº Ù†ÛŒØ² Ù†ÛŒÚ†Û’ Ù†Û’ Ùˆ ÙˆØ§Ø± ÙˆØ§Ø³Ø·Û’ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ§Ù„Ø§ ÙˆØ§Ù„ÙˆÚº ÙˆØ§Ù„ÛŒ ÙˆØ§Ù„Û’ ÙˆØ§Û ÙˆØ¬Û ÙˆØ±Ù†Û ÙˆØ¹Ù„ÛŒÚ©Ù… ÙˆØºÛŒØ±Û ÙˆÙ„Û’
ÙˆÚ¯Ø±Ù†Û ÙˆÛ ÙˆÛØ§Úº ÙˆÛÛŒ ÙˆÛÛŒÚº ÙˆÛŒØ³Ø§ ÙˆÛŒØ³Û’ ÙˆÛŒÚº Ù¾Ø§Ø³ Ù¾Ø§ÛŒØ§ Ù¾Ø± Ù¾Ø³ Ù¾Ù„ÛŒØ² Ù¾ÙˆÙ† Ù¾ÙˆÙ†Ø§ Ù¾ÙˆÙ†ÛŒ Ù¾ÙˆÙ†Û’ Ù¾Ú¾Ø§Ú¯Ù†
Ù¾Ú¾Ø± Ù¾Û Ù¾ÛØ± Ù¾ÛÙ„Ø§ Ù¾ÛÙ„ÛŒ Ù¾ÛÙ„Û’ Ù¾ÛŒØ± Ù¾ÛŒÚ†Ú¾Û’ Ú†Ø§ÛØ¦Û’ Ú†Ø§ÛØªÛ’ Ú†Ø§ÛÛŒØ¦Û’ Ú†Ø§ÛÛ’ Ú†Ù„Ø§ Ú†Ù„Ùˆ Ú†Ù„ÛŒÚº Ú†Ù„Û’ Ú†Ù†Ø§Ú†Û Ú†Ù†Ø¯ Ú†ÙˆÙ†Ú©Û
Ú†ÙˆØºÙ†ÛŒ Ú†Ú©ÛŒ Ú†Ú©ÛŒÚº Ú†Ú©Û’ Ú†ÛØ§Ø±Ø´Ù†Ø¨Û Ú†ÛŒØª ÚˆØ§Ù„Ù†Ø§ ÚˆØ§Ù„Ù†ÛŒ ÚˆØ§Ù„Ù†Û’ ÚˆØ§Ù„Û’ Ú©Ø¦Û’ Ú©Ø§ Ú©Ø§ØªÚ© Ú©Ø§Ø´ Ú©Ø¨ Ú©Ø¨Ú¾ÛŒ Ú©Ø¯Ú¾Ø± Ú©Ø±
Ú©Ø±ØªØ§ Ú©Ø±ØªÛŒ Ú©Ø±ØªÛ’ Ú©Ø±Ù… Ú©Ø±Ù†Ø§ Ú©Ø±Ù†Û’ Ú©Ø±Ùˆ Ú©Ø±ÛŒÚº Ú©Ø±Û’ Ú©Ø³ Ú©Ø³ÛŒ Ú©Ø³Û’ Ú©Ù„ Ú©Ù… Ú©Ù† Ú©Ù†ÛÛŒÚº Ú©Ùˆ Ú©ÙˆØ¦ÛŒ Ú©ÙˆÙ†
Ú©ÙˆÙ†Ø³Ø§ Ú©ÙˆÙ†Ø³Û’ Ú©Ú†Ú¾ Ú©Û Ú©ÛØ§ Ú©ÛØ§Úº Ú©ÛÛ Ú©ÛÛŒ Ú©ÛÛŒÚº Ú©ÛÛ’ Ú©ÛŒ Ú©ÛŒØ§ Ú©ÛŒØ³Ø§ Ú©ÛŒØ³Û’ Ú©ÛŒÙˆÙ†Ú©Ø± Ú©ÛŒÙˆÙ†Ú©Û Ú©ÛŒÙˆÚº Ú©ÛŒÛ’
Ú©Û’ Ú¯Ø¦ÛŒ Ú¯Ø¦Û’ Ú¯Ø§ Ú¯Ø±Ù…Ø§ Ú¯Ø±Ù…ÛŒ Ú¯Ù†Ø§ Ú¯Ùˆ Ú¯ÙˆÛŒØ§ Ú¯Ú¾Ù†Ù¹Ø§ Ú¯Ú¾Ù†Ù¹ÙˆÚº Ú¯Ú¾Ù†Ù¹Û’ Ú¯ÛŒ Ú¯ÛŒØ§ ÛØ§Ø¦ÛŒÚº ÛØ§Ø¦Û’ ÛØ§Ú‘ ÛØ§Úº ÛØ±
ÛØ±Ú†Ù†Ø¯ ÛØ±Ú¯Ø² ÛØ²Ø§Ø± ÛÙØªÛ ÛÙ… ÛÙ…Ø§Ø±Ø§ ÛÙ…Ø§Ø±ÛŒ ÛÙ…Ø§Ø±Û’ ÛÙ…ÛŒ ÛÙ…ÛŒÚº ÛÙˆ ÛÙˆØ¦ÛŒ ÛÙˆØ¦ÛŒÚº ÛÙˆØ¦Û’ ÛÙˆØ§ ÛÙˆØ¨ÛÙˆ ÛÙˆØªØ§ ÛÙˆØªÛŒ
ÛÙˆØªÛŒÚº ÛÙˆØªÛ’ ÛÙˆÙ†Ø§ ÛÙˆÙ†Ú¯Û’ ÛÙˆÙ†ÛŒ ÛÙˆÙ†Û’ ÛÙˆÚº ÛÛŒ ÛÛŒÙ„Ùˆ ÛÛŒÚº ÛÛ’ ÛŒØ§ ÛŒØ§Øª ÛŒØ¹Ù†ÛŒ ÛŒÚ© ÛŒÛ ÛŒÛØ§Úº ÛŒÛÛŒ ÛŒÛÛŒÚº
""".split())

final_stopwords = set(stopwords_from_file + stopwords_from_csv)
final_stopwords.update(stop_words) 

# 5. Loading Lemmatization Dictionary
urdu_dict = pd.read_csv("Dictionary_final.csv")
lemma_dict = pd.Series(urdu_dict.Lemma.values, index=urdu_dict.Word).to_dict()

# 6. Loading the trained model and vectorizer with caching
@st.cache_resource
def load_model_and_vectorizer():
    model = joblib.load('urdu_sentiment_model.pkl')
    tfidf = joblib.load('urdu_sentiment_tfidf.pkl')
    return model, tfidf

model, tfidf = load_model_and_vectorizer()

# 7. Preprocessing Functions
def remove_numbers(text):
    return re.sub(r'\d+', '', text)

def remove_emoji(text):
    emoji_pattern = re.compile("["  
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"  
        u"\U000024C2-\U0001F251"  
        u"\U0001F900-\U0001F9FF"  # additional emojis
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_hash_sign(text):
    return re.sub(r'#', '', text)  

def remove_mentions(text):
    return re.sub(r'@\w+', '', text)

def remove_url(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return pattern.sub(r'', text)

def remove_english_words(text):
    return re.sub(r'\b[a-zA-Z]+\b', '', text)

def remove_diacritics(text):
    urdu_diacritics  = ['Ù', 'Ù°', 'Ù', 'Ù', 'Ù‹', 'Ù']
    for letter in text:
        if letter in urdu_diacritics:
            text = text.replace(letter, '')
    return text

def remove_stopwords(text, final_stopwords):
    new_text = []
    for word in text.split():
        if word not in final_stopwords:
            new_text.append(word)
    return " ".join(new_text)

def lemmatize_text(text, lemma_dict):
    words = text.split()  
    lemmatized_words = []
    
    for word in words:
        if word in lemma_dict:
            lemmatized_words.append(lemma_dict[word])
        else:
            lemmatized_words.append(word)
    return " ".join(lemmatized_words)

# 8. Prediction Function with Confidence Scores
def predict_sarcasm(user_input):
    # Preprocess the input text
    user_input = remove_numbers(user_input)
    user_input = remove_emoji(user_input)
    user_input = remove_hash_sign(user_input)
    user_input = remove_mentions(user_input)
    user_input = remove_url(user_input)
    user_input = remove_english_words(user_input)
    user_input = remove_diacritics(user_input)
    user_input = remove_punctuation(user_input)
    user_input = remove_stopwords(user_input, final_stopwords)
    user_input = lemmatize_text(user_input, lemma_dict)

    # Transform the text into the TF-IDF vector
    text_vector = tfidf.transform([user_input]).toarray()

    # Predict sarcasm probabilities
    prediction_proba = model.predict_proba(text_vector)[0]
    prob_non_sarcastic = prediction_proba[0]
    prob_sarcastic = prediction_proba[1]

    # Determine prediction based on higher probability
    prediction = "Sarcastic" if prob_sarcastic > prob_non_sarcastic else "Non-Sarcastic"

    return prediction, prob_sarcastic, prob_non_sarcastic

# 9. Sidebar: Example Sentences
st.sidebar.header("Example Sentences")

# Sarcastic Urdu Sentences
sarcastic_examples = [
    "Ø´Ø§Ø¨Ø§Ø´! Ø¢Ø¬ Ø¢Ù¾ Ù†Û’ Ú©Ù…Ø§Ù„ Ú©Ø± Ø¯ÛŒØ§",
    "ÙˆØ§Û! ØªÙ… Ù†Û’ ØªÙˆ Ù¾ÙˆØ±Ø§ Ø¯Ù† Ø³ÙˆÙ†Û’ Ú©Ø§ Ø±ÛŒÚ©Ø§Ø±Úˆ ØªÙˆÚ‘ Ø¯ÛŒØ§ØŒ Ø¨ÛØª Ù…Ø­Ù†ØªÛŒ ÛÙˆÛ”",
    "ÙˆØ§Ù‚Ø¹ÛŒØŒ ØªÙ…ÛÛŒÚº Ø³Ø¨ Ú©Ú†Ú¾ Ù…Ø¹Ù„ÙˆÙ… ÛÙˆØªØ§ ÛÛ’ØŒ ØªÙ… ØªÙˆ Ø§Ù†Ø³Ø§Ø¦ÛŒÚ©Ù„ÙˆÙ¾ÛŒÚˆÛŒØ§ ÛÙˆ",
    "Ú©Ù…Ø§Ù„ ÛÛ’ØŒ ØªÙ… Ú©Ø¨Ú¾ÛŒ ØºÙ„Ø·ÛŒ Ù†ÛÛŒÚº Ú©Ø±ØªÛ’ØŒ Ø¯Ù†ÛŒØ§ Ù…ÛŒÚº ØªÙ…ÛØ§Ø±ÛŒ Ù…Ø«Ø§Ù„ Ù†ÛÛŒÚº!",
    "ØªÙ…ÛØ§Ø±ÛŒ Ø±ÙØªØ§Ø± ØªÙˆ Ø§ÛŒØ³ÛŒ ÛÛ’ Ø¬ÛŒØ³Û’ ØªÙ… Ø¯ÙˆÚ‘ Ù…ÛŒÚº Ø­ØµÛ Ù„Û’ Ø±ÛÛ’ ÛÙˆ!"
]

# Non-Sarcastic Urdu Sentences
non_sarcastic_examples = [
    "Ù…Ø¬Ú¾Û’ Ø§Ø±Ø¯Ùˆ Ú©ØªØ§Ø¨ÛŒÚº Ù¾Ú‘Ú¾Ù†Ø§ Ø¨ÛØª Ù¾Ø³Ù†Ø¯ ÛÛ’",
    "Ø¢Ø¬ Ù…ÙˆØ³Ù… Ø¨ÛØª Ø®ÙˆØ¨ØµÙˆØ±Øª ÛÛ’Û”",
    "Ø¢Ù¾ Ú©ÛŒ Ù…Ø¯Ø¯ Ú©Û’ Ù„ÛŒÛ’ Ø´Ú©Ø±ÛŒÛÛ”",
    "Ù…ÛŒÚº Ù†Û’ Ø¢Ø¬ Ø§ÛŒÚ© Ù†Ø¦ÛŒ Ú†ÛŒØ² Ø³ÛŒÚ©Ú¾ÛŒ",
    "Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯ØŒ Ú¯Ú¾Ø± Ù…ÛŒÚº ØªØ´Ø±ÛŒÙ Ù„Ø§Ø¦ÛŒÚºÛ”"
]

# Display Sarcastic Examples
st.sidebar.subheader("Sarcastic Urdu Sentences")
sarcastic_choice = st.sidebar.selectbox("Choose a sarcastic example", sarcastic_examples)

# Display Non-Sarcastic Examples
st.sidebar.subheader("Non-Sarcastic Urdu Sentences")
non_sarcastic_choice = st.sidebar.selectbox("Choose a non-sarcastic example", non_sarcastic_examples)

# Button to use Sarcastic Example
if st.sidebar.button("Use Sarcastic Example"):
    st.session_state['user_input'] = sarcastic_choice

# Button to use Non-Sarcastic Example
if st.sidebar.button("Use Non-Sarcastic Example"):
    st.session_state['user_input'] = non_sarcastic_choice

# 10. Display selected example in the main text input
if 'user_input' in st.session_state:
    user_input_text = st.text_input("Your sentence here", value=st.session_state['user_input'], placeholder="Type in Urdu...", key="input")
else:
    user_input_text = st.text_input("Your sentence here", placeholder="Type in Urdu...", key="input")

# 11. Main Input and Prediction
if user_input_text:
    with st.spinner('Analyzing...'):
        prediction, prob_sarcastic, prob_non_sarcastic = predict_sarcasm(user_input_text)
    
    # Display Prediction
    st.success(f"**Prediction:** {prediction}")
    
    # Display Confidence Score as a Single Progress Bar
    st.markdown("### Confidence Score:")
    confidence_percentage = prob_sarcastic * 100 if prediction == "Sarcastic" else prob_non_sarcastic * 100
    confidence_label = "Sarcastic Confidence" if prediction == "Sarcastic" else "Non-Sarcastic Confidence"
    
    # Display the confidence bar with percentage
    st.markdown(f"**{confidence_label}:** {confidence_percentage:.2f}%")
    st.progress(float(confidence_percentage)/100)  # Convert to 0-1 scale
    
    # Visual feedback using color blocks
    if prediction == "Sarcastic":
        st.markdown(
            "<div style='background-color: #2c2c2c; padding: 20px; border-radius: 5px; color: #ffcccb; font-size: 18px; text-align: center;'>"
            "ÛŒÛ Ø¬Ù…Ù„Û Ø·Ù†Ø²ÛŒÛ ÛÛ’ ğŸ˜"
            "</div>",
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            "<div style='background-color: #2c2c2c; padding: 20px; border-radius: 5px; color: #ccffcc; font-size: 18px; text-align: center;'>"
            "ÛŒÛ Ø¬Ù…Ù„Û ØºÛŒØ± Ø·Ù†Ø²ÛŒÛ ÛÛ’ ğŸ˜Š"
            "</div>",
            unsafe_allow_html=True
        )

# 12. Footer: About Section in English
st.sidebar.markdown("## About")
st.sidebar.markdown("""
This app utilizes a trained model to analyze Urdu sentences and determine whether they are sarcastic or non-sarcastic. The model processes the input text by cleaning and lemmatizing it, transforming it into a TF-IDF vector, and then predicting the sentiment. The confidence score indicates the likelihood of the sentence being sarcastic or non-sarcastic.
""")
