import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import urduhack
from urduhack.preprocessing import normalize_whitespace
from urduhack.preprocessing import remove_punctuation
from urduhack.preprocessing import remove_accents
import demoji
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
import gensim
from sklearn.model_selection import train_test_split
from nltk import ngrams
from collections import Counter 
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import streamlit as st
import joblib

# Load the trained model and vectorizer
model = joblib.load('urdu_sentiment_model.pkl')
tfidf = joblib.load('urdu_sentiment_tfidf.pkl')

# Streamlit app setup with modern design

st.set_page_config(
    page_title="Sarcasm Detection App", 
    page_icon="ğŸ§ ", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# Custom CSS for centering content
st.markdown("""
    <style>
    /* Center the title, subheader, and input box */
    .centered-content {
        display: flex;
        justify-content: center;
        align-items: center;
        flex-direction: column;
    }

    /* Center and style the buttons */
    .stButton > button {
        background-color: #00aaff;
        color: white;
        border-radius: 10px;
        width: 200px;
        margin: 20px auto;
        display: block;
    }

    /* Centering text input box */
    div[role='textbox'] {
        margin-left: auto;
        margin-right: auto;
        width: 50%;
        text-align: center;
    }

    /* Style for title */
    h1, h2, h3 {
        text-align: center;
        color: #00aaff;
    }

    body {
        background-color: #f0f2f6;
    }

    </style>
""", unsafe_allow_html=True)

# Title and description with centered layout
st.markdown('<div class="centered-content">', unsafe_allow_html=True)
st.title("ğŸ§  Urdu Sarcasm Detection")
st.subheader("Analyze Urdu text for sarcasm with AI-powered detection")
st.markdown('</div>', unsafe_allow_html=True)

# Making a list of stopwords
with open("stopwords-ur.txt", 'r', encoding='utf-8') as file:
        stopwords_from_file = file.read().splitlines()

stopdf=pd.read_csv("urdu_stopwords.csv",encoding='utf-8')
stopwords_from_csv = stopdf["stopword"].tolist()

# Additional stopwords
stop_words = set("""

 Ø¢ Ø¢Ø¦ÛŒ Ø¢Ø¦ÛŒÚº Ø¢Ø¦Û’ Ø¢ØªØ§ Ø¢ØªÛŒ Ø¢ØªÛ’ Ø¢Ø¯Ø§Ø¨ Ø¢Ø¯Ú¾ Ø¢Ø¯Ú¾Ø§ Ø¢Ø¯Ú¾ÛŒ Ø¢Ø¯Ú¾Û’ Ø¢Ø³ Ø¢Ù…Ø¯ÛŒØ¯ Ø¢Ù†Ø§ Ø¢Ù†Ø³Û Ø¢Ù†ÛŒ Ø¢Ù†Û’
 Ø¢Ù¾ Ø¢Ú¯Û’ Ø¢Û Ø¢ÛØ§ Ø¢ÛŒØ§ Ø§Ø¨ Ø§Ø¨Ú¾ÛŒ Ø§Ø¨Û’ Ø§ØªÙˆØ§Ø± Ø§Ø±Ø¨ Ø§Ø±Ø¨ÙˆÛŒÚº Ø§Ø±Û’ Ø§Ø³ Ø§Ø³Ú©Ø§ Ø§Ø³Ú©ÛŒ Ø§Ø³Ú©Û’ Ø§Ø³ÛŒ Ø§Ø³Û’ Ø§Ù
 Ø§ÙÙˆÛ Ø§Ù„Ø§ÙˆÙ„ Ø§Ù„Ø¨ØªÛ Ø§Ù„Ø«Ø§Ù†ÛŒ Ø§Ù„Ø­Ø±Ø§Ù… Ø§Ù„Ø³Ù„Ø§Ù… Ø§Ù„Ù Ø§Ù„Ù…Ú©Ø±Ù… Ø§Ù† Ø§Ù†Ø¯Ø± Ø§Ù†Ú©Ø§ Ø§Ù†Ú©ÛŒ Ø§Ù†Ú©Û’ Ø§Ù†ÛÙˆÚº Ø§Ù†ÛÛŒ Ø§Ù†ÛÛŒÚº Ø§ÙˆØ¦Û’ Ø§ÙˆØ±
 Ø§ÙˆÙ¾Ø± Ø§ÙˆÛÙˆ Ø§Ù¾ Ø§Ù¾Ù†Ø§ Ø§Ù¾Ù†ÙˆÚº Ø§Ù¾Ù†ÛŒ Ø§Ù¾Ù†Û’ Ø§Ù¾Ù†Û’Ø¢Ù¾ Ø§Ú©Ø¨Ø± Ø§Ú©Ø«Ø± Ø§Ú¯Ø± Ø§Ú¯Ø±Ú†Û Ø§Ú¯Ø³Øª Ø§ÛØ§ÛØ§ Ø§ÛŒØ³Ø§ Ø§ÛŒØ³ÛŒ Ø§ÛŒØ³Û’ Ø§ÛŒÚ© Ø¨Ø§Ø¦ÛŒÚº
 Ø¨Ø§Ø± Ø¨Ø§Ø±Û’ Ø¨Ø§Ù„Ú©Ù„ Ø¨Ø§ÙˆØ¬ÙˆØ¯ Ø¨Ø§ÛØ± Ø¨Ø¬ Ø¨Ø¬Û’ Ø¨Ø®ÛŒØ± Ø¨Ø±Ø³Ø§Øª Ø¨Ø´Ø±Ø·ÛŒÚ©Û Ø¨Ø¹Ø¯ Ø¨Ø¹Ø¶ Ø¨ØºÛŒØ± Ø¨Ù„Ú©Û Ø¨Ù† Ø¨Ù†Ø§ Ø¨Ù†Ø§Ø¤ Ø¨Ù†Ø¯
 Ø¨Ú‘ÛŒ Ø¨Ú¾Ø± Ø¨Ú¾Ø±ÛŒÚº Ø¨Ú¾ÛŒ Ø¨ÛØ§Ø± Ø¨ÛØª Ø¨ÛØªØ± Ø¨ÛŒÚ¯Ù… ØªØ§Ú©Û ØªØ§ÛÙ… ØªØ¨ ØªØ¬Ú¾ ØªØ¬Ú¾ÛŒ ØªØ¬Ú¾Û’ ØªØ±Ø§ ØªØ±ÛŒ ØªÙ„Ú© ØªÙ… ØªÙ…Ø§Ù…
 ØªÙ…ÛØ§Ø±Ø§ ØªÙ…ÛØ§Ø±ÙˆÚº ØªÙ…ÛØ§Ø±ÛŒ ØªÙ…ÛØ§Ø±Û’ ØªÙ…ÛÛŒÚº ØªÙˆ ØªÚ© ØªÚ¾Ø§ ØªÚ¾ÛŒ ØªÚ¾ÛŒÚº ØªÚ¾Û’ ØªÛØ§Ø¦ÛŒ ØªÛŒØ±Ø§ ØªÛŒØ±ÛŒ ØªÛŒØ±Û’ ØªÛŒÙ† Ø¬Ø§ Ø¬Ø§Ø¤
 Ø¬Ø§Ø¦ÛŒÚº Ø¬Ø§Ø¦Û’ Ø¬Ø§ØªØ§ Ø¬Ø§ØªÛŒ Ø¬Ø§ØªÛ’ Ø¬Ø§Ù†ÛŒ Ø¬Ø§Ù†Û’ Ø¬Ø¨ Ø¬Ø¨Ú©Û Ø¬Ø¯Ú¾Ø± Ø¬Ø³ Ø¬Ø³Û’ Ø¬Ù† Ø¬Ù†Ø§Ø¨ Ø¬Ù†ÛÙˆÚº Ø¬Ù†ÛÛŒÚº Ø¬Ùˆ Ø¬ÛØ§Úº Ø¬ÛŒ
 Ø¬ÛŒØ³Ø§ Ø¬ÛŒØ³ÙˆÚº Ø¬ÛŒØ³ÛŒ Ø¬ÛŒØ³Û’ Ø¬ÛŒÙ¹Ú¾ Ø­Ø§Ù„Ø§Ù†Ú©Û Ø­Ø§Ù„Ø§Úº Ø­ØµÛ Ø­Ø¶Ø±Øª Ø®Ø§Ø·Ø± Ø®Ø§Ù„ÛŒ Ø®Ø¯Ø§ Ø®Ø²Ø§Úº Ø®ÙˆØ§Û Ø®ÙˆØ¨ Ø®ÙˆØ¯ Ø¯Ø§Ø¦ÛŒÚº Ø¯Ø±Ù…ÛŒØ§Ù†
 Ø¯Ø±ÛŒÚº Ø¯Ùˆ Ø¯ÙˆØ±Ø§Ù† Ø¯ÙˆØ³Ø±Ø§ Ø¯ÙˆØ³Ø±ÙˆÚº Ø¯ÙˆØ³Ø±ÛŒ Ø¯ÙˆØ´Ù†Ø¨Û Ø¯ÙˆÚº Ø¯Ú©Ú¾Ø§Ø¦ÛŒÚº Ø¯Ú¯Ù†Ø§ Ø¯ÛŒ Ø¯ÛŒØ¦Û’ Ø¯ÛŒØ§ Ø¯ÛŒØªØ§ Ø¯ÛŒØªÛŒ Ø¯ÛŒØªÛ’ Ø¯ÛŒØ± Ø¯ÛŒÙ†Ø§ Ø¯ÛŒÙ†ÛŒ
 Ø¯ÛŒÙ†Û’ Ø¯ÛŒÚ©Ú¾Ùˆ Ø¯ÛŒÚº Ø¯ÛŒÛ’ Ø¯Û’ Ø°Ø±ÛŒØ¹Û’ Ø±Ú©Ú¾Ø§ Ø±Ú©Ú¾ØªØ§ Ø±Ú©Ú¾ØªÛŒ Ø±Ú©Ú¾ØªÛ’ Ø±Ú©Ú¾Ù†Ø§ Ø±Ú©Ú¾Ù†ÛŒ Ø±Ú©Ú¾Ù†Û’ Ø±Ú©Ú¾Ùˆ Ø±Ú©Ú¾ÛŒ Ø±Ú©Ú¾Û’ Ø±Û Ø±ÛØ§
 Ø±ÛØªØ§ Ø±ÛØªÛŒ Ø±ÛØªÛ’ Ø±ÛÙ†Ø§ Ø±ÛÙ†ÛŒ Ø±ÛÙ†Û’ Ø±ÛÙˆ Ø±ÛÛŒ Ø±ÛÛŒÚº Ø±ÛÛ’ Ø³Ø§ØªÚ¾ Ø³Ø§Ù…Ù†Û’ Ø³Ø§Ú‘Ú¾Û’ Ø³Ø¨ Ø³Ø¨Ú¾ÛŒ Ø³Ø±Ø§Ø³Ø± Ø³Ù„Ø§Ù… Ø³Ù…ÛŒØª Ø³ÙˆØ§
 Ø³ÙˆØ§Ø¦Û’ Ø³Ú©Ø§ Ø³Ú©ØªØ§ Ø³Ú©ØªÛ’ Ø³Û Ø³ÛÛŒ Ø³ÛŒ Ø³Û’ Ø´Ø§Ù… Ø´Ø§ÛŒØ¯ Ø´Ú©Ø±ÛŒÛ ØµØ§Ø­Ø¨ ØµØ§Ø­Ø¨Û ØµØ±Ù Ø¶Ø±ÙˆØ± Ø·Ø±Ø­ Ø·Ø±Ù Ø·ÙˆØ±
 Ø¹Ù„Ø§ÙˆÛ Ø¹ÛŒÙ† ÙØ±ÙˆØ±ÛŒ ÙÙ‚Ø· ÙÙ„Ø§Úº ÙÛŒ Ù‚Ø¨Ù„ Ù‚Ø·Ø§ Ù„Ø¦Û’ Ù„Ø§Ø¦ÛŒ Ù„Ø§Ø¦Û’ Ù„Ø§ØªØ§ Ù„Ø§ØªÛŒ Ù„Ø§ØªÛ’ Ù„Ø§Ù†Ø§ Ù„Ø§Ù†ÛŒ Ù„Ø§Ù†Û’ Ù„Ø§ÛŒØ§ Ù„Ùˆ
 Ù„ÙˆØ¬ÛŒ Ù„ÙˆÚ¯ÙˆÚº Ù„Ú¯ Ù„Ú¯Ø§ Ù„Ú¯ØªØ§ Ù„Ú¯ØªÛŒ Ù„Ú¯ÛŒ Ù„Ú¯ÛŒÚº Ù„Ú¯Û’ Ù„ÛØ°Ø§ Ù„ÛŒ Ù„ÛŒØ§ Ù„ÛŒØªØ§ Ù„ÛŒØªÛŒ Ù„ÛŒØªÛ’ Ù„ÛŒÚ©Ù† Ù„ÛŒÚº Ù„ÛŒÛ’
 Ù„Û’ Ù…Ø§Ø³ÙˆØ§ Ù…Øª Ù…Ø¬Ú¾ Ù…Ø¬Ú¾ÛŒ Ù…Ø¬Ú¾Û’ Ù…Ø­ØªØ±Ù… Ù…Ø­ØªØ±Ù…Û Ù…Ø­ØªØ±Ù…ÛŒ Ù…Ø­Ø¶ Ù…Ø±Ø§ Ù…Ø±Ø­Ø¨Ø§ Ù…Ø±ÛŒ Ù…Ø±Û’ Ù…Ø²ÛŒØ¯ Ù…Ø³ Ù…Ø³Ø² Ù…Ø³Ù¹Ø± Ù…Ø·Ø§Ø¨Ù‚
 Ù…Ø·Ù„Ù‚ Ù…Ù„ Ù…Ù†Ù¹ Ù…Ù†Ù¹ÙˆÚº Ù…Ú©Ø±Ù…ÛŒ Ù…Ú¯Ø± Ù…Ú¯Ú¾Ø± Ù…ÛØ±Ø¨Ø§Ù†ÛŒ Ù…ÛŒØ±Ø§ Ù…ÛŒØ±ÙˆÚº Ù…ÛŒØ±ÛŒ Ù…ÛŒØ±Û’ Ù…ÛŒÚº Ù†Ø§ Ù†Ø²Ø¯ÛŒÚ© Ù†Ù…Ø§ Ù†Ùˆ Ù†ÙˆÙ…Ø¨Ø±
 Ù†Û Ù†ÛÛŒÚº Ù†ÛŒØ² Ù†ÛŒÚ†Û’ Ù†Û’ Ùˆ ÙˆØ§Ø± ÙˆØ§Ø³Ø·Û’ ÙˆØ§Ù‚Ø¹ÛŒ ÙˆØ§Ù„Ø§ ÙˆØ§Ù„ÙˆÚº ÙˆØ§Ù„ÛŒ ÙˆØ§Ù„Û’ ÙˆØ§Û ÙˆØ¬Û ÙˆØ±Ù†Û ÙˆØ¹Ù„ÛŒÚ©Ù… ÙˆØºÛŒØ±Û ÙˆÙ„Û’
 ÙˆÚ¯Ø±Ù†Û ÙˆÛ ÙˆÛØ§Úº ÙˆÛÛŒ ÙˆÛÛŒÚº ÙˆÛŒØ³Ø§ ÙˆÛŒØ³Û’ ÙˆÛŒÚº Ù¾Ø§Ø³ Ù¾Ø§ÛŒØ§ Ù¾Ø± Ù¾Ø³ Ù¾Ù„ÛŒØ² Ù¾ÙˆÙ† Ù¾ÙˆÙ†Ø§ Ù¾ÙˆÙ†ÛŒ Ù¾ÙˆÙ†Û’ Ù¾Ú¾Ø§Ú¯Ù†
 Ù¾Ú¾Ø± Ù¾Û Ù¾ÛØ± Ù¾ÛÙ„Ø§ Ù¾ÛÙ„ÛŒ Ù¾ÛÙ„Û’ Ù¾ÛŒØ± Ù¾ÛŒÚ†Ú¾Û’ Ú†Ø§ÛØ¦Û’ Ú†Ø§ÛØªÛ’ Ú†Ø§ÛÛŒØ¦Û’ Ú†Ø§ÛÛ’ Ú†Ù„Ø§ Ú†Ù„Ùˆ Ú†Ù„ÛŒÚº Ú†Ù„Û’ Ú†Ù†Ø§Ú†Û Ú†Ù†Ø¯ Ú†ÙˆÙ†Ú©Û
 Ú†ÙˆÚ¯Ù†ÛŒ Ú†Ú©ÛŒ Ú†Ú©ÛŒÚº Ú†Ú©Û’ Ú†ÛØ§Ø±Ø´Ù†Ø¨Û Ú†ÛŒØª ÚˆØ§Ù„Ù†Ø§ ÚˆØ§Ù„Ù†ÛŒ ÚˆØ§Ù„Ù†Û’ ÚˆØ§Ù„Û’ Ú©Ø¦Û’ Ú©Ø§ Ú©Ø§ØªÚ© Ú©Ø§Ø´ Ú©Ø¨ Ú©Ø¨Ú¾ÛŒ Ú©Ø¯Ú¾Ø± Ú©Ø±
 Ú©Ø±ØªØ§ Ú©Ø±ØªÛŒ Ú©Ø±ØªÛ’ Ú©Ø±Ù… Ú©Ø±Ù†Ø§ Ú©Ø±Ù†Û’ Ú©Ø±Ùˆ Ú©Ø±ÛŒÚº Ú©Ø±Û’ Ú©Ø³ Ú©Ø³ÛŒ Ú©Ø³Û’ Ú©Ù„ Ú©Ù… Ú©Ù† Ú©Ù†ÛÛŒÚº Ú©Ùˆ Ú©ÙˆØ¦ÛŒ Ú©ÙˆÙ†
 Ú©ÙˆÙ†Ø³Ø§ Ú©ÙˆÙ†Ø³Û’ Ú©Ú†Ú¾ Ú©Û Ú©ÛØ§ Ú©ÛØ§Úº Ú©ÛÛ Ú©ÛÛŒ Ú©ÛÛŒÚº Ú©ÛÛ’ Ú©ÛŒ Ú©ÛŒØ§ Ú©ÛŒØ³Ø§ Ú©ÛŒØ³Û’ Ú©ÛŒÙˆÙ†Ú©Ø± Ú©ÛŒÙˆÙ†Ú©Û Ú©ÛŒÙˆÚº Ú©ÛŒÛ’
 Ú©Û’ Ú¯Ø¦ÛŒ Ú¯Ø¦Û’ Ú¯Ø§ Ú¯Ø±Ù…Ø§ Ú¯Ø±Ù…ÛŒ Ú¯Ù†Ø§ Ú¯Ùˆ Ú¯ÙˆÛŒØ§ Ú¯Ú¾Ù†Ù¹Ø§ Ú¯Ú¾Ù†Ù¹ÙˆÚº Ú¯Ú¾Ù†Ù¹Û’ Ú¯ÛŒ Ú¯ÛŒØ§ ÛØ§Ø¦ÛŒÚº ÛØ§Ø¦Û’ ÛØ§Ú‘ ÛØ§Úº ÛØ±
 ÛØ±Ú†Ù†Ø¯ ÛØ±Ú¯Ø² ÛØ²Ø§Ø± ÛÙØªÛ ÛÙ… ÛÙ…Ø§Ø±Ø§ ÛÙ…Ø§Ø±ÛŒ ÛÙ…Ø§Ø±Û’ ÛÙ…ÛŒ ÛÙ…ÛŒÚº ÛÙˆ ÛÙˆØ¦ÛŒ ÛÙˆØ¦ÛŒÚº ÛÙˆØ¦Û’ ÛÙˆØ§ ÛÙˆØ¨ÛÙˆ ÛÙˆØªØ§ ÛÙˆØªÛŒ
 ÛÙˆØªÛŒÚº ÛÙˆØªÛ’ ÛÙˆÙ†Ø§ ÛÙˆÙ†Ú¯Û’ ÛÙˆÙ†ÛŒ ÛÙˆÙ†Û’ ÛÙˆÚº ÛÛŒ ÛÛŒÙ„Ùˆ ÛÛŒÚº ÛÛ’ ÛŒØ§ ÛŒØ§Øª ÛŒØ¹Ù†ÛŒ ÛŒÚ© ÛŒÛ ÛŒÛØ§Úº ÛŒÛÛŒ ÛŒÛÛŒÚº


""".split())

final_stopwords = set(stopwords_from_file + stopwords_from_csv)
final_stopwords.update(stop_words) 


urdu_dict=pd.read_csv("Dictionary_final.csv")
lemma_dict = pd.Series(urdu_dict.Lemma.values, index=urdu_dict.Word).to_dict()



# Function for preprocessing and predicting sarcasm
def remove_numbers(text):
    return re.sub(r'\d+', '', text)

def remove_emoji(text):
    emoji_pattern = re.compile("["  
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"  
        u"\U000024C2-\U0001F251"  
        u"\U0001F900-\U0001F9FF"  # additional emojis
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_hash_sign(text):
    return re.sub(r'#', '', text)  

def remove_mentions(text):
    return re.sub(r'@\w+', '', text)

def remove_url(text):
    pattern = re.compile(r'https?://\S+|www\.\S+')
    return pattern.sub(r'', text)

def remove_english_words(text):
    return re.sub(r'\b[a-zA-Z]+\b', '', text)

def remove_diacritics(text):
    urdu_diacritics  = ['Ù', 'Ù°', 'Ù', 'Ù', 'Ù‹', 'Ù']
    for letter in text:
        if letter in urdu_diacritics:
            text = text.replace(letter, '')
    return text

# Assuming stopwords are loaded correctly as per your original code
def remove_stopwords(text, final_stopwords):
    new_text = []
    for word in text.split():
        if word not in final_stopwords:
            new_text.append(word)
    return " ".join(new_text)

# Assuming lemmatization dictionary loaded correctly
def lemmatize_text(text, lemma_dict):
    words = text.split()  
    lemmatized_words = []
    
    for word in words:
        if word in lemma_dict:
            lemmatized_words.append(lemma_dict[word])
        else:
            lemmatized_words.append(word)
    return " ".join(lemmatized_words)

def predict_sarcasm(user_input):
    # Preprocess the input text
    user_input = remove_numbers(user_input)
    user_input = remove_emoji(user_input)
    user_input = remove_hash_sign(user_input)
    user_input = remove_mentions(user_input)
    user_input = remove_url(user_input)
    user_input = remove_english_words(user_input)
    user_input = remove_diacritics(user_input)
    user_input = remove_punctuation(user_input)
    user_input = remove_stopwords(user_input, final_stopwords)
    user_input = lemmatize_text(user_input, lemma_dict)

    # Transform the text into the TF-IDF vector
    text_vector = tfidf.transform([user_input]).toarray()

    # Predict sarcasm (1 for sarcastic, 0 for non-sarcastic)
    prediction = model.predict(text_vector)

    if prediction == 1:
        return "Sarcastic"
    else:
        return "Non-Sarcastic"

# Main input box
st.markdown("### Enter an Urdu sentence to analyze:")
user_input = st.text_input("Your sentence here", placeholder="Type in Urdu...")
if user_input:
    with st.spinner('Analyzing...'):
        prediction = predict_sarcasm(user_input)
    st.success(f"Prediction: {prediction}")

    # Visual feedback using color blocks
# Visual feedback using color blocks
    if prediction == "Sarcastic":
        st.markdown(
            "<div style='background-color: #2c2c2c; padding: 20px; border-radius: 5px; color: #ffcccb; font-size: 18px; text-align: center;'>"
            "This sentence is Sarcastic ğŸ˜"
            "</div>",
            unsafe_allow_html=True
        )
    else:
        st.markdown(
            "<div style='background-color: #2c2c2c; padding: 20px; border-radius: 5px; color: #ccffcc; font-size: 18px; text-align: center;'>"
            "This sentence is Non-Sarcastic ğŸ˜Š"
            "</div>",
            unsafe_allow_html=True
        )


